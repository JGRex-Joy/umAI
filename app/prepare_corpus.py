import os
import json
import numpy as np
from pathlib import Path
from services.docx_loader import load_docx
from services.chunker import chunk_text
from services.embedder import embed_texts
from services.vector_search import build_faiss_index

DOCS = {
    "–ú–∞–Ω–∞—Å": "app/data/manas.docx",
    "–°—ã–Ω–≥–∞–Ω –ö—ã–ª—ã—á": "app/data/syngan_kylych.docx"
}
OUTPUT_DIR = "embeddings"
FAISS_FILE = f"{OUTPUT_DIR}/index.faiss"
META_FILE = f"{OUTPUT_DIR}/metadata.json"

def build_corpus():
    corpus = []
    global_id = 0
    for book_name, path in DOCS.items():
        text = load_docx(path)
        chunks = chunk_text(text)
        for c in chunks:
            corpus.append({
                "id": global_id,
                "text": c,
                "source": book_name
            })
            global_id += 1
    return corpus

if __name__ == "__main__":
    print("üìñ –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ—Ä–ø—É—Å–∞‚Ä¶")
    corpus = build_corpus()
    print(f"‚úÖ –ß–∞–Ω–∫–æ–≤: {len(corpus)}")

    print("üîé –≠–º–±–µ–¥–¥–∏–Ω–≥–∏‚Ä¶")
    texts = [c["text"] for c in corpus]
    embeddings = embed_texts(texts)

    print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º‚Ä¶")
    Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)
    build_faiss_index(embeddings, corpus, FAISS_FILE, META_FILE)

    print("üéâ –ì–æ—Ç–æ–≤–æ! index.faiss + metadata.json —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.")
